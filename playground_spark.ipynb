{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "# We don't need to create SparkContext everytime if we are dealing with DataFrmes - only for Spark 2.X and above\n",
    "# SparkSession however is required\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession\n",
    "# Spark session get or create,Spark is designed to have only one active Spark session per JVM (Java Virtual Machine). \n",
    "# If you're running Spark code in an interactive environment or script, \n",
    "# it's common to see this warning when you attempt to create a new Spark session after one has already been established.\n",
    "ss = SparkSession.builder.appName('playground-test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.159:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>playground-test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8929152070>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|        USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         2|     Jane|   Smith|janesmith@example...|Female| 34|     Canada|    2023-02-01|2023-02-12|      89.9|\n",
      "|         3|      Bob|   Brown|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|2023-01-25|     305.6|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|        USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         5|     Mark|   Black|markblack@example...|  Male| 37|  Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "|         6|    Emily|   Green|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|2023-03-10|    255.25|\n",
      "|         7|    James|    Hall|jameshall@example...|  Male| 41|         UK|    2023-01-30|2023-02-02|     320.8|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|        USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|         9|  William| Johnson|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|2023-01-30|     180.4|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|        USA|    2023-02-15|2023-02-18|     240.9|\n",
      "|        11|     Noah|     Lee| noahlee@example.com|  Male| 22|         UK|    2023-01-05|2023-01-09|      88.0|\n",
      "|        12|      Mia|   Brown|miabrown@example.com|Female| 27|     Canada|    2023-02-25|2023-02-26|     175.5|\n",
      "|        13|    Ethan|   Clark|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|2023-01-19|     290.3|\n",
      "|        14|     Emma|  Wilson|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|2023-03-03|     310.2|\n",
      "|        15|  Michael|  Martin|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|  2023-02-|      NULL|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# There are multiple ways we can read csv.\n",
    "# Using read.option.csv and using read.csv(and then providing options here)\n",
    "df = ss.read.csv('test.csv', header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refernce Spark: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- AccountCreated: date (nullable = true)\n",
      " |-- LastLogin: string (nullable = true)\n",
      " |-- TotalSpend: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the Schema of the DF same as pandas .info() / .describe()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+---------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|  Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+---------+--------------+----------+----------+\n",
      "|         5|     Mark|   Black|markblack@example...|  Male| 37|Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "+----------+---------+--------+--------------------+------+---+---------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It samples fraction of total rows - meaning gives out randomly fraction % of total rows\n",
    "# using the second optional argument seed\n",
    "df.sample(fraction = 0.2, seed = 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|         1|\n",
      "|         3|\n",
      "|         4|\n",
      "|         5|\n",
      "|         6|\n",
      "|        10|\n",
      "|        11|\n",
      "|        13|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select is used to select particular colums\n",
    "### df.columns returns a list of columns\n",
    "df.select( df.columns[0]).sample(0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|    USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|    USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|    USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|    USA|    2023-02-15|2023-02-18|     240.9|\n",
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can use filter and where interchangeably\n",
    "df.where(df.Country=='USA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|    USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|    USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|    USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|    USA|    2023-02-15|2023-02-18|     240.9|\n",
      "+----------+---------+--------+--------------------+------+---+-------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Country=='USA').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop - drops the coloumn specified and returns a new dataframe, \n",
    "# it doesn't make modifications inplace\n",
    "df2 = df.drop(\"LastLogin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|        USA|    2023-01-01|     120.5|\n",
      "|         2|     Jane|   Smith|janesmith@example...|Female| 34|     Canada|    2023-02-01|      89.9|\n",
      "|         3|      Bob|   Brown|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|     305.6|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|        USA|    2023-03-15|     150.0|\n",
      "|         5|     Mark|   Black|markblack@example...|  Male| 37|  Australia|    2023-02-10|     204.7|\n",
      "|         6|    Emily|   Green|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|    255.25|\n",
      "|         7|    James|    Hall|jameshall@example...|  Male| 41|         UK|    2023-01-30|     320.8|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|        USA|    2023-03-12|      95.6|\n",
      "|         9|  William| Johnson|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|     180.4|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|        USA|    2023-02-15|     240.9|\n",
      "|        11|     Noah|     Lee| noahlee@example.com|  Male| 22|         UK|    2023-01-05|      88.0|\n",
      "|        12|      Mia|   Brown|miabrown@example.com|Female| 27|     Canada|    2023-02-25|     175.5|\n",
      "|        13|    Ethan|   Clark|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|     290.3|\n",
      "|        14|     Emma|  Wilson|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|     310.2|\n",
      "|        15|  Michael|  Martin|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|      NULL|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropna - drops all the rows on specified condition how - can take two values\n",
    "# viz any and all.\n",
    "#We can pass optional subset parameter, to apply how only on those subset of cols\n",
    "df3 = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|        USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         2|     Jane|   Smith|janesmith@example...|Female| 34|     Canada|    2023-02-01|2023-02-12|      89.9|\n",
      "|         3|      Bob|   Brown|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|2023-01-25|     305.6|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|        USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         5|     Mark|   Black|markblack@example...|  Male| 37|  Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "|         6|    Emily|   Green|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|2023-03-10|    255.25|\n",
      "|         7|    James|    Hall|jameshall@example...|  Male| 41|         UK|    2023-01-30|2023-02-02|     320.8|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|        USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|         9|  William| Johnson|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|2023-01-30|     180.4|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|        USA|    2023-02-15|2023-02-18|     240.9|\n",
      "|        11|     Noah|     Lee| noahlee@example.com|  Male| 22|         UK|    2023-01-05|2023-01-09|      88.0|\n",
      "|        12|      Mia|   Brown|miabrown@example.com|Female| 27|     Canada|    2023-02-25|2023-02-26|     175.5|\n",
      "|        13|    Ethan|   Clark|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|2023-01-19|     290.3|\n",
      "|        14|     Emma|  Wilson|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|2023-03-03|     310.2|\n",
      "|        15|  Michael|  Martin|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|  2023-02-|      NULL|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CustomerID', 'int'),\n",
       " ('FirstName', 'string'),\n",
       " ('LastName', 'string'),\n",
       " ('Email', 'string'),\n",
       " ('Gender', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Country', 'string'),\n",
       " ('AccountCreated', 'date'),\n",
       " ('LastLogin', 'string'),\n",
       " ('TotalSpend', 'double')]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtypes returns a tuple of col, dtype of col\n",
    "df3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|         1|     John|     Doe| johndoe@example.com|  Male| 28|        USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         2|     Jane|   Smith|janesmith@example...|Female| 34|     Canada|    2023-02-01|2023-02-12|      89.9|\n",
      "|         3|      Bob|   Brown|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|2023-01-25|     305.6|\n",
      "|         4|     Lisa|   White|lisawhite@example...|Female| 23|        USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         5|     Mark|   Black|markblack@example...|  Male| 37|  Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "|         6|    Emily|   Green|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|2023-03-10|    255.25|\n",
      "|         7|    James|    Hall|jameshall@example...|  Male| 41|         UK|    2023-01-30|2023-02-02|     320.8|\n",
      "|         8|   Olivia|   Scott|oliviascott@examp...|Female| 19|        USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|         9|  William| Johnson|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|2023-01-30|     180.4|\n",
      "|        10|      Ava|   Jones|avajones@example.com|Female| 31|        USA|    2023-02-15|2023-02-18|     240.9|\n",
      "|        11|     Noah|     Lee| noahlee@example.com|  Male| 22|         UK|    2023-01-05|2023-01-09|      88.0|\n",
      "|        12|      Mia|   Brown|miabrown@example.com|Female| 27|     Canada|    2023-02-25|2023-02-26|     175.5|\n",
      "|        13|    Ethan|   Clark|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|2023-01-19|     290.3|\n",
      "|        14|     Emma|  Wilson|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|2023-03-03|     310.2|\n",
      "|        15|  Michael|  Martin|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|  2023-02-|      10.0|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.fillna({'TotalSpend':10}).show()\n",
    "# Ref: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.fillna.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "john\n",
      "jane\n",
      "bob\n",
      "lisa\n",
      "mark\n",
      "emily\n",
      "james\n",
      "olivia\n",
      "william\n",
      "ava\n",
      "noah\n",
      "mia\n",
      "ethan\n",
      "emma\n",
      "michael\n"
     ]
    }
   ],
   "source": [
    "# We can apply UDF to dataframe by using foreach\n",
    "# ref :https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.foreach.html\n",
    "def func(df):\n",
    "    print(df.FirstName.lower())\n",
    "\n",
    "df3.foreach(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .collect() method in PySpark is used to retrieve all the data from a DataFrame or RDD and bring it into the driver program's memory, which is generally not recommended for large datasets. This method returns the data as a list in the driver program.\n",
    "\n",
    "While .collect() can be useful for small datasets that fit into the memory of the driver program, it should be used with caution for large datasets. Here are some considerations:\n",
    "\n",
    "Memory Usage: .collect() brings all the data into the driver program's memory. If the dataset is large, it can lead to memory issues or even crashes.\n",
    "\n",
    "Performance: Transferring large amounts of data from the distributed cluster to the driver program can be time-consuming and can affect the performance of your application.\n",
    "\n",
    "Out of Memory Errors: If the dataset is too large to fit into the available memory of the driver program, it may result in out-of-memory errors.\n",
    "\n",
    "In general, it's recommended to perform operations on distributed data using PySpark's transformations and actions without collecting data into the driver program unless necessary. If you need to analyze or visualize the data in the driver program, consider using techniques like sampling or aggregating the data before calling .collect().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john\n",
      "jane\n",
      "bob\n",
      "lisa\n",
      "mark\n",
      "emily\n",
      "james\n",
      "olivia\n",
      "william\n",
      "ava\n",
      "noah\n",
      "mia\n",
      "ethan\n",
      "emma\n",
      "michael\n"
     ]
    }
   ],
   "source": [
    "for elem in df3.select(df3.FirstName).collect():\n",
    "    print(elem.asDict()['FirstName'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn addsd new Coloumn to dataframe returns new dataframe, if col name is same\n",
    "# it will replace the existing col\n",
    "from pyspark.sql.functions import lower\n",
    "df3 = df3.withColumn(\"FirstName\", lower(df3[\"FirstName\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|         1|     john|     Doe| johndoe@example.com|  Male| 28|        USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         2|     jane|   Smith|janesmith@example...|Female| 34|     Canada|    2023-02-01|2023-02-12|      89.9|\n",
      "|         3|      bob|   Brown|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|2023-01-25|     305.6|\n",
      "|         4|     lisa|   White|lisawhite@example...|Female| 23|        USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         5|     mark|   Black|markblack@example...|  Male| 37|  Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "|         6|    emily|   Green|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|2023-03-10|    255.25|\n",
      "|         7|    james|    Hall|jameshall@example...|  Male| 41|         UK|    2023-01-30|2023-02-02|     320.8|\n",
      "|         8|   olivia|   Scott|oliviascott@examp...|Female| 19|        USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|         9|  william| Johnson|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|2023-01-30|     180.4|\n",
      "|        10|      ava|   Jones|avajones@example.com|Female| 31|        USA|    2023-02-15|2023-02-18|     240.9|\n",
      "|        11|     noah|     Lee| noahlee@example.com|  Male| 22|         UK|    2023-01-05|2023-01-09|      88.0|\n",
      "|        12|      mia|   Brown|miabrown@example.com|Female| 27|     Canada|    2023-02-25|2023-02-26|     175.5|\n",
      "|        13|    ethan|   Clark|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|2023-01-19|     290.3|\n",
      "|        14|     emma|  Wilson|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|2023-03-03|     310.2|\n",
      "|        15|  michael|  Martin|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|  2023-02-|      NULL|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|CustomerID|FirstName|LastName|               Email|Gender|Age|    Country|AccountCreated| LastLogin|TotalSpend|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "|         1|     john|     DOE| johndoe@example.com|  Male| 28|        USA|    2023-01-01|2023-01-15|     120.5|\n",
      "|         2|     jane|   SMITH|janesmith@example...|Female| 34|     Canada|    2023-02-01|2023-02-12|      89.9|\n",
      "|         3|      bob|   BROWN|bobbrown@example.com|  Male| 45|         UK|    2023-01-20|2023-01-25|     305.6|\n",
      "|         4|     lisa|   WHITE|lisawhite@example...|Female| 23|        USA|    2023-03-15|2023-03-16|     150.0|\n",
      "|         5|     mark|   BLACK|markblack@example...|  Male| 37|  Australia|    2023-02-10|2023-02-20|     204.7|\n",
      "|         6|    emily|   GREEN|emilygreen@exampl...|Female| 29|New Zealand|    2023-03-05|2023-03-10|    255.25|\n",
      "|         7|    james|    HALL|jameshall@example...|  Male| 41|         UK|    2023-01-30|2023-02-02|     320.8|\n",
      "|         8|   olivia|   SCOTT|oliviascott@examp...|Female| 19|        USA|    2023-03-12|2023-03-13|      95.6|\n",
      "|         9|  william| JOHNSON|williamjohnson@ex...|  Male| 36|     Canada|    2023-01-22|2023-01-30|     180.4|\n",
      "|        10|      ava|   JONES|avajones@example.com|Female| 31|        USA|    2023-02-15|2023-02-18|     240.9|\n",
      "|        11|     noah|     LEE| noahlee@example.com|  Male| 22|         UK|    2023-01-05|2023-01-09|      88.0|\n",
      "|        12|      mia|   BROWN|miabrown@example.com|Female| 27|     Canada|    2023-02-25|2023-02-26|     175.5|\n",
      "|        13|    ethan|   CLARK|ethanclark@exampl...|  Male| 39|  Australia|    2023-01-18|2023-01-19|     290.3|\n",
      "|        14|     emma|  WILSON|emmawilson@exampl...|Female| 42|New Zealand|    2023-03-01|2023-03-03|     310.2|\n",
      "|        15|  michael|  MARTIN|michaelmartin@exa...|  Male| 33|         UK|    2023-02-05|  2023-02-|      NULL|\n",
      "+----------+---------+--------+--------------------+------+---+-----------+--------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now withColumn is used to transform only one col, but withColoumns can be used to\n",
    "# apply transformations to many columns at once.\n",
    "from pyspark.sql.functions import upper\n",
    "df3.withColumns({\"FirstName\": lower(df3['FirstName']), \"LastName\": upper(df3['LastName'])} ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Bracket Notation (df[\"ColumnName\"]):\n",
    "\n",
    "This is the most explicit and versatile way to reference a column.\n",
    "It allows you to use column names with spaces or special characters, e.g., df[\"First Name\"].\n",
    "It is used for both selecting and updating columns.\n",
    "\n",
    "Using Dot Notation (df.ColumnName):\n",
    "\n",
    "This notation is convenient and concise.\n",
    "It is suitable for column names without spaces or special characters.\n",
    "It is primarily used for selecting columns.\n",
    "\n",
    "Using col Function (col(\"ColumnName\")):\n",
    "\n",
    "PySpark provides a col function from the pyspark.sql.functions module, which can be used to reference a column.\n",
    "It is commonly used when applying functions to columns or in the context of expressions.\n",
    "\n",
    "Using expr Function (expr(\"ColumnName\")):\n",
    "\n",
    "The expr function is used to create expressions involving columns.\n",
    "It allows you to use SQL-like expressions.\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr(\"ColumnName + 1\")\n",
    "Using select Method (df.select(\"ColumnName\")):\n",
    "\n",
    "The select method is used to select one or more columns from a DataFrame.\n",
    "It takes either column names as strings or column expressions.\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "df.select(\"ColumnName\")\n",
    "In your provided code snippet, you are using both bracket notation (df3['FirstName'] and df3['LastName']) and the withColumns method. The choice between these notations often depends on the context, personal preference, or the specific operation being performed. For example, using df[\"ColumnName\"] is generally safe and explicit, while dot notation (df.ColumnName) is concise but may not work with certain column names. It's recommended to use the notation that suits your specific use case and ensures readability and clarity in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      Full Name|\n",
      "+---------------+\n",
      "|       John Doe|\n",
      "|     Jane Smith|\n",
      "|      Bob Brown|\n",
      "|     Lisa White|\n",
      "|     Mark Black|\n",
      "|    Emily Green|\n",
      "|     James Hall|\n",
      "|   Olivia Scott|\n",
      "|William Johnson|\n",
      "|      Ava Jones|\n",
      "|       Noah Lee|\n",
      "|      Mia Brown|\n",
      "|    Ethan Clark|\n",
      "|    Emma Wilson|\n",
      "| Michael Martin|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expr is another useful funciton which stands for expression.\n",
    "# We can write SQL like functions into exper and can trasnform data using pysaprk\n",
    "from pyspark.sql.functions import lower, initcap, upper, concat, col, expr\n",
    "df.withColumns({\"FirstName\": initcap(df[\"FirstName\"]),\n",
    "                     \"LastName\": lower(df[\"LastName\"]),\n",
    "                     \"Country\": upper(df[\"Country\"]),\n",
    "                   \"Full Name\": expr(\"FirstName || ' ' ||LastName\")}).select('Full Name').show()\n",
    "df.select(df.Country).distinct().count()\n",
    "x = df.fillna({\"TotalSpend\" : 50, \"LastLogin\": '2024'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|    Country|\n",
      "+-----------+\n",
      "|        USA|\n",
      "|         UK|\n",
      "|     Canada|\n",
      "|New Zealand|\n",
      "|  Australia|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.select(df3.Country).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|    Country|count|\n",
      "+-----------+-----+\n",
      "|        USA|    4|\n",
      "|         UK|    4|\n",
      "|     Canada|    3|\n",
      "|New Zealand|    2|\n",
      "|  Australia|    2|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Country\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------+---------------+\n",
      "|    Country|max(CustomerID)|max(Age)|max(TotalSpend)|\n",
      "+-----------+---------------+--------+---------------+\n",
      "|        USA|             10|      31|          240.9|\n",
      "|         UK|             15|      45|          320.8|\n",
      "|     Canada|             12|      36|          180.4|\n",
      "|New Zealand|             14|      42|          310.2|\n",
      "|  Australia|             13|      39|          290.3|\n",
      "+-----------+---------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"Country\").max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pandas and PySpark are both powerful tools for working with data, but they serve different purposes and have distinct advantages depending on the scale of your data and the requirements of your tasks.\n",
    "\n",
    "Pandas:\n",
    "Single-node Processing: Pandas is designed for in-memory data processing on a single machine. It's excellent for smaller to moderately sized datasets that can fit into the memory of a single machine.\n",
    "\n",
    "Ease of Use: Pandas provides an easy-to-use and expressive API, making it a popular choice for data analysis and manipulation. It is widely used in the data science and analytics community.\n",
    "\n",
    "Rich Functionality: Pandas supports a broad range of operations, including data cleaning, transformation, aggregation, and visualization.\n",
    "\n",
    "PySpark:\n",
    "Distributed Processing: PySpark, on the other hand, is designed for distributed data processing and can scale horizontally to handle large datasets across a cluster of machines. It uses the Hadoop Distributed File System (HDFS) for storage and Apache Spark for computation.\n",
    "\n",
    "Big Data Processing: PySpark is well-suited for processing big data where the data is distributed across multiple nodes. It can handle data sizes that exceed the capacity of a single machine.\n",
    "\n",
    "Resilient Distributed Datasets (RDDs): PySpark uses RDDs, an abstraction for distributed data, making it fault-tolerant and scalable.\n",
    "\n",
    "Parallelism: PySpark inherently supports parallelism, allowing it to perform operations on distributed data in parallel across a cluster of machines.\n",
    "\n",
    "When to Use Which:\n",
    "Pandas: Use Pandas for smaller to medium-sized datasets that fit into memory and for interactive data analysis and exploration.\n",
    "\n",
    "PySpark: Use PySpark when dealing with large-scale distributed datasets that require horizontal scaling. It's particularly useful in a big data processing environment.\n",
    "\n",
    "Combination: In some cases, you might use Pandas for initial data exploration and preprocessing on a subset of the data and then switch to PySpark for processing the entire dataset in a distributed manner.\n",
    "\n",
    "In summary, the choice between Pandas and PySpark depends on the scale of your data and the computational resources available. For big data processing, PySpark's distributed nature is a significant advantage, while Pandas is more suitable for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|FirstName|avg(age)|\n",
      "+---------+--------+\n",
      "|      ava|    31.0|\n",
      "|      bob|    45.0|\n",
      "|    emily|    29.0|\n",
      "|     emma|    42.0|\n",
      "|    ethan|    39.0|\n",
      "|    james|    41.0|\n",
      "|     jane|    34.0|\n",
      "|     john|    28.0|\n",
      "|     lisa|    23.0|\n",
      "|     mark|    37.0|\n",
      "|      mia|    27.0|\n",
      "|  michael|    33.0|\n",
      "|     noah|    22.0|\n",
      "|   olivia|    19.0|\n",
      "|  william|    36.0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(\"FirstName\").avg('age').alias(\"Age\").sort(\"FirstName\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.groupBy(\"Country\").agg({\"Age\":\"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    Country|               Age|\n",
      "+-----------+------------------+\n",
      "|        USA|             25.25|\n",
      "|         UK|             35.25|\n",
      "|     Canada|32.333333333333336|\n",
      "|New Zealand|              35.5|\n",
      "|  Australia|              38.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.withColumnRenamed(\"avg(Age)\", \"Age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When we reassign a df result to another df it creates a new dataframe,\n",
    "# and to verify it we can usee .rdd.id() function to get its unique id\n",
    "df3.rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.id()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
